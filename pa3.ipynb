{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp, inf, log\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_data(train=True):\n",
    "    X = []\n",
    "    y = []\n",
    "    for f in os.listdir('train/face' if train else 'test/face'):\n",
    "        X.append(imread(f).ravel())\n",
    "        y.append(1)\n",
    "\n",
    "    for f in os.listdir('train/non-face' if train else 'test/non-face'):\n",
    "        X.append(imread(f).ravel())\n",
    "        y.append(0)\n",
    "    \n",
    "    return pd.DataFrame(X, columns=range(len(X[0]))), pd.Series(y)\n",
    "\n",
    "\n",
    "def fetch_train():\n",
    "    return fetch_data(True)    \n",
    "\n",
    "\n",
    "def fetch_test():\n",
    "    return fetch_data(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(h, y):\n",
    "    # Accuracy = TP + TN / TP +TN +FP +FN\n",
    "    tp = list(map(all, zip(h, y))).count(True)\n",
    "    tf = list(map(any, zip(h, y))).count(False)\n",
    "    return (tp + tf) / h.shape[0]\n",
    "\n",
    "\n",
    "def precision(h, y):\n",
    "    # Precision = TP / (TP + FP)\n",
    "    tp = list(map(all, zip(h, y))).count(True)\n",
    "    fp = list(zip(h, y)).count((1, 0))\n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "\n",
    "def recall(h, y):\n",
    "    # Recall = TP /(TP + FN)\n",
    "    tp = list(map(all, zip(h, y))).count(True)\n",
    "    fn = list(zip(h, y)).count((0, 1))\n",
    "    if tp + fn == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "\n",
    "def f1(h, y):\n",
    "    # F1 = 2 * (p * r / (p + r))\n",
    "    p = precision(h, y)\n",
    "    r = recall(h, y)\n",
    "    if p+r == 0.0:\n",
    "        return 0.0\n",
    "    return 2 * (p * r / (p + r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.yprior = {}\n",
    "        self.y_indices = {}\n",
    "        self.xcounts = {}\n",
    "\n",
    "        self.class_counts = yvals = y.value_counts()\n",
    "        for z in yvals:\n",
    "            self.yprior[z] = yvals[z] / yvals.sum()\n",
    "            self.y_indices[z] = [z == v for v in self.y]\n",
    "\n",
    "        for z in yvals:\n",
    "            # count x values by\n",
    "            self.xcounts[z] = [self.X[c].count_values().to_dict() for c in self.X]\n",
    "\n",
    "    def px(self, X, y):\n",
    "        logpx = 0\n",
    "        for x in X:\n",
    "            logpx += log((self.xcounts[y].get(x, 0) + 1) / (self.class_counts[y] + 256))\n",
    "        \n",
    "        return logpx\n",
    "\n",
    "    def py(self, y):\n",
    "        return self.yprior[y]\n",
    "\n",
    "    def classify(self, X):\n",
    "        maxp = -inf\n",
    "        cls = None\n",
    "        for y in self.yprior.keys():\n",
    "            p = self.py(y) + self.px(X, y)\n",
    "            if p > maxp:\n",
    "                maxp = p\n",
    "                cls = y\n",
    "\n",
    "        return cls\n",
    "\n",
    "    def classify_all(self, X):\n",
    "        return [self.classify(r) for _, r in X.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # print(z)\n",
    "    try:\n",
    "        return 1 / (1 + exp(-z))\n",
    "    except OverflowError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def hw(X, w):\n",
    "    return np.array(list(map(sigmoid, np.matmul(X, w))))\n",
    "\n",
    "\n",
    "def cost(h, y):\n",
    "    # cost (hw(x), y) = − [y log(hw (x)) + (1 − y) log(1 − hw (x))]\n",
    "    # return - (y * log(h) + (1-y) * log(1-h))\n",
    "    if y == 0:\n",
    "        if h == 1.0:\n",
    "            h -= 1e-6\n",
    "        return -log(1-h)\n",
    "    else:\n",
    "        if h == 0.0:\n",
    "            h += 1e-6\n",
    "        return -log(h)\n",
    "\n",
    "\n",
    "def costs(hyp, y):\n",
    "    return np.array([cost(h, target) for (h, target) in zip(hyp, y)])\n",
    "\n",
    "\n",
    "def batch_gradient_descent(X, y, nEpoch, alpha, lmbda=None):\n",
    "    w = np.ones((X.shape[1],), dtype=np.float128)\n",
    "    loss = -1\n",
    "    for _ in np.arange(0, nEpoch):\n",
    "        hypothesis = hw(X, w)\n",
    "        loss = sum(costs(hypothesis, y))  # + sum(1/X.shape[0] * (w ** 2))\n",
    "\n",
    "        error = hypothesis - y\n",
    "        gradient = X.T.dot(error)\n",
    "\n",
    "        if lmbda is None:\n",
    "            w -= alpha * gradient\n",
    "        else:\n",
    "            w[0] -= alpha * gradient[0]\n",
    "            w[1:] -= alpha * (gradient[1:] - lmbda * w[1:])\n",
    "\n",
    "    print(\"epoch #{} alpha {} lambda {}, loss={:.7f}\".format(nEpoch, alpha, lmbda, loss))\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X, y, nEpoch, alpha, lmbda=None):\n",
    "    w = np.ones((X.shape[1],), dtype=np.float128)\n",
    "    loss = -1\n",
    "    for _ in np.arange(0, nEpoch):\n",
    "        idx = random.randrange(X.shape[0])\n",
    "        hypothesis = sigmoid(np.matmul(X[idx], w))\n",
    "        loss = cost(hypothesis, y[idx])\n",
    "\n",
    "        error = hypothesis - y\n",
    "        gradient = X[idx].T.dot(error)\n",
    "\n",
    "        if lmbda is None:\n",
    "            w -= alpha * gradient\n",
    "        else:\n",
    "            w[0] -= alpha * gradient[0]\n",
    "            w[1:] -= alpha * (gradient[1:] - lmbda * w[1:])\n",
    "\n",
    "    print(\"epoch #{} alpha {} lambda {}, loss={:.7f}\".format(nEpoch, alpha, lmbda, loss))\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, trainY = fetch_train()\n",
    "testX, testY = fetch_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Naive Bayes\n",
    "naive_bayes = NaiveBayes(trainX, trainY)\n",
    "nb_results = naive_bayes.classify(testX)\n",
    "\n",
    "nb_accuracy = accuracy(nb_results, testY)\n",
    "nb_precision = precision(nb_results, testY)\n",
    "nb_recall = recall(nb_results, testY)\n",
    "nb_f1 = f1(nb_results, testY)\n",
    "nb_correct = [nb_results[i] == testY[i] for i in range(testX.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Batch Gradient Descent\n",
    "bgd_results = batch_gradient_descent(trainX, trainY, 100, 0.1, 1)\n",
    "\n",
    "bgd_accuracy = accuracy(bgd_results, testY)\n",
    "bgd_precision = precision(bgd_results, testY)\n",
    "bgd_recall = recall(bgd_results, testY)\n",
    "bgd_f1 = f1(bgd_results, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stochastic Gradient Descent\n",
    "sgd_results = stochastic_gradient_descent(trainX, trainY['y'], 1000, 0.1, 1)\n",
    "\n",
    "sgd_accuracy = accuracy(sgd_results, testY['y'])\n",
    "sgd_precision = precision(sgd_results, testY['y'])\n",
    "sgd_recall = recall(sgd_results, testY['y'])\n",
    "sgd_f1 = f1(sgd_results, testY['y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
